import logging

import duckdb
import pendulum
from airflow import DAG
from airflow.models import Variable
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator

OWNER = "sergei"
DAG_ID = "raw_api_to_s3"

LAYER = "raw"
SOURCE = "earthquake"

ACCESS_KEY = Variable.get("access_key")
SECRET_KEY = Variable.get("secret_key")

LONG_DESCRIPTION = """
# DAG для загрузки данных о землетрясениях из USGS API в S3 (MinIO)

## Назначение
Этот DAG ежедневно загружает данные о землетрясениях с публичного API Геологической службы США (USGS) 
и сохраняет их в S3-совместимое хранилище (MinIO) в формате Parquet.

## Источник данных
  `https://earthquake.usgs.gov/fdsnws/event/1/query?format=csv&starttime={date}&endtime={date}`
- Данные включают: время, координаты, магнитуду, глубину и другую информацию о землетрясениях

## Настройка хранилища
- Данные сохраняются в MinIO (S3)
- Путь сохранения: `s3://prod/raw/earthquake/<YYYY-MM-DD>/<YYYY-MM-DD>_00-00-00.gz.parquet`
- Формат: Parquet с GZIP-сжатием

## Параметры выполнения
- Расписание: ежедневно в 05:00 UTC
- Поддержка дозагрузки (catchup)
- Количество повторных попыток: 2
- Интервал между попытками: 30 минут
"""

SHORT_DESCRIPTION = "Ежедневная загрузка данных о землетрясениях из USGS API в MinIO (S3)"

args = {
    "owner": OWNER,
    "start_date": pendulum.datetime(2025, 8, 1, tz="Europe/Moscow"),
    "catchup": True,
    "retries": 2,
    "retry_delay": pendulum.duration(minutes=30),
}


def get_dates(**context) -> tuple[str, str]:
    """"""
    start_date = context["data_interval_start"].format("YYYY-MM-DD")
    end_date = context["data_interval_end"].format("YYYY-MM-DD")

    return start_date, end_date


def get_and_transfer_api_data_to_s3(**context):

    start_date, end_date = get_dates(**context)
    logging.info(f"Start load for dates: {start_date}/{end_date}")
    con = duckdb.connect()

    con.sql(
        f"""
        SET TIMEZONE='UTC';
        INSTALL httpfs;
        LOAD httpfs;
        SET s3_url_style = 'path';
        SET s3_endpoint = 'minio:9000';
        SET s3_access_key_id = '{ACCESS_KEY}';
        SET s3_secret_access_key = '{SECRET_KEY}';
        SET s3_use_ssl = FALSE;

        COPY
        (
            SELECT
                *
            FROM
                read_csv_auto('https://earthquake.usgs.gov/fdsnws/event/1/query?format=csv&starttime={start_date}&endtime={end_date}') AS res
        ) TO 's3://prod/{LAYER}/{SOURCE}/{start_date}/{start_date}_00-00-00.gz.parquet';

        """,
    )

    con.close()
    logging.info(f"Download for date success: {start_date}")


with DAG(
    dag_id=DAG_ID,
    schedule_interval="0 5 * * *",
    default_args=args,
    tags=["s3", "raw"],
    description=SHORT_DESCRIPTION,
    concurrency=1,
    max_active_tasks=1,
    max_active_runs=1,
) as dag:
    dag.doc_md = LONG_DESCRIPTION

    start = EmptyOperator(
        task_id="start",
    )

    get_and_transfer_api_data_to_s3 = PythonOperator(
        task_id="get_and_transfer_api_data_to_s3",
        python_callable=get_and_transfer_api_data_to_s3,
    )

    end = EmptyOperator(
        task_id="end",
    )

    start >> get_and_transfer_api_data_to_s3 >> end


